{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9007022e",
   "metadata": {},
   "source": [
    "# Q1. Theory and Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44daf051",
   "metadata": {},
   "source": [
    "1.  Batch normalization is a technique used in artificial neural networks to improve the training speed, stability, and generalization performance of the model. It normalizes the activations of each layer by adjusting and scaling them. Here's an explanation of the concept of batch normalization in the context of artificial neural networks:\n",
    "\n",
    "Normalization:\n",
    "\n",
    "In neural networks, the activations of each layer can vary widely during training due to changes in the input distribution and parameters of the network. This can lead to slower convergence and make training unstable.\n",
    "Batch normalization addresses this issue by normalizing the activations of each layer. It subtracts the mean and divides by the standard deviation of the activations within a mini-batch during training.\n",
    "Algorithm:\n",
    "\n",
    "Given a mini-batch of activations \n",
    "{\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "}\n",
    "{x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,...,x \n",
    "m\n",
    "​\n",
    " } for a particular layer, batch normalization first calculates the mean \n",
    "�\n",
    "μ and standard deviation \n",
    "�\n",
    "σ of the mini-batch.\n",
    "It then normalizes the activations using the formula:\n",
    "�\n",
    "^\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "2\n",
    "+\n",
    "�\n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " = \n",
    "σ \n",
    "2\n",
    " +ϵ\n",
    "​\n",
    " \n",
    "x \n",
    "i\n",
    "​\n",
    " −μ\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "^\n",
    "�\n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    "  is the normalized activation, \n",
    "�\n",
    "ϵ is a small constant (usually added for numerical stability), and \n",
    "�\n",
    "i indexes the activations within the mini-batch.\n",
    "After normalization, batch normalization introduces two learnable parameters, \n",
    "�\n",
    "γ (scale) and \n",
    "�\n",
    "β (shift), which allow the network to learn the optimal scaling and shifting of the normalized activations:\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "^\n",
    "�\n",
    "+\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    " =γ \n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " +β\n",
    "Benefits:\n",
    "\n",
    "Accelerated Training: Batch normalization helps accelerate the training process by reducing internal covariate shift. This allows for higher learning rates and faster convergence.\n",
    "Regularization: Batch normalization acts as a form of regularization, reducing the need for other regularization techniques such as dropout, which can sometimes discard useful information.\n",
    "Stabilized Gradients: By normalizing activations, batch normalization helps stabilize the gradients during backpropagation, which can mitigate the vanishing and exploding gradient problems.\n",
    "Improved Generalization: Batch normalization often leads to improved generalization performance on unseen data by reducing overfitting.\n",
    "Usage:\n",
    "\n",
    "Batch normalization is commonly applied to the activations of each layer in a neural network, typically before the activation function.\n",
    "It is widely used in various types of neural network architectures, including fully connected networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee75ed",
   "metadata": {},
   "source": [
    "2.  Using batch normalization during training offers several benefits that contribute to improved performance and stability of artificial neural networks (ANNs). Here are some of the key benefits:\n",
    "\n",
    "Improved Convergence Speed:\n",
    "Batch normalization helps in stabilizing and accelerating the convergence of the training process. By normalizing the activations within each mini-batch, it reduces the internal covariate shift, allowing the network to converge faster. This results in quicker training times, especially for deep networks.\n",
    "\n",
    "Stabilized Training:\n",
    "Batch normalization helps in stabilizing the training process by reducing the sensitivity to the initialization of model parameters. It mitigates the vanishing and exploding gradient problems by ensuring that the activations are centered and have a consistent scale, which leads to more stable gradients during backpropagation.\n",
    "\n",
    "Reduction of Internal Covariate Shift:\n",
    "Internal covariate shift refers to the change in the distribution of network activations due to parameter updates during training. Batch normalization addresses this issue by normalizing the activations using mini-batch statistics, which helps in maintaining a more consistent distribution of activations throughout the network layers. This stabilizes the training process and allows for the use of higher learning rates without the risk of divergence.\n",
    "\n",
    "Regularization Effect:\n",
    "Batch normalization acts as a form of regularization by adding noise to the activations during training. This noise helps prevent overfitting by adding a slight amount of randomness to the training process, similar to dropout regularization. This regularization effect can lead to better generalization performance on unseen data.\n",
    "\n",
    "Robustness to Parameter Initialization:\n",
    "Batch normalization makes the training process less sensitive to the choice of initial parameter values. Since the activations are normalized within each mini-batch, the network becomes less dependent on the scale and distribution of the initial weights. This allows for more flexibility in choosing initialization methods and facilitates training of deeper networks.\n",
    "\n",
    "Facilitation of Deeper Networks:\n",
    "Batch normalization enables the training of deeper neural networks by mitigating the vanishing gradient problem. It allows gradients to flow more smoothly through the network, even in architectures with many layers. This facilitates the training of deeper and more complex models, leading to improved performance on challenging tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990991e1",
   "metadata": {},
   "source": [
    "3.  Batch normalization (BN) is a technique used in training neural networks to improve the stability and speed of convergence. It works by normalizing the activations of each layer and introducing learnable parameters to scale and shift these normalized activations. Here's a detailed explanation of how batch normalization works:\n",
    "\n",
    "Normalization Step:\n",
    "\n",
    "Given a mini-batch of activations \n",
    "{\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "}\n",
    "{x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,...,x \n",
    "m\n",
    "​\n",
    " } for a particular layer, where \n",
    "�\n",
    "m is the batch size, batch normalization first calculates the mean \n",
    "�\n",
    "μ and standard deviation \n",
    "�\n",
    "σ of the activations within the mini-batch.\n",
    "The mean and standard deviation are computed along each feature dimension separately, resulting in a mean and standard deviation for each feature.\n",
    "The activations are then normalized using these statistics. For each activation \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    " , the normalized value \n",
    "�\n",
    "^\n",
    "�\n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    "  is calculated as:\n",
    "�\n",
    "^\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "2\n",
    "+\n",
    "�\n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " = \n",
    "σ \n",
    "2\n",
    " +ϵ\n",
    "​\n",
    " \n",
    "x \n",
    "i\n",
    "​\n",
    " −μ\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "ϵ is a small constant (typically added for numerical stability to avoid division by zero).\n",
    "Learnable Parameters:\n",
    "\n",
    "After normalization, batch normalization introduces two learnable parameters, \n",
    "�\n",
    "γ and \n",
    "�\n",
    "β, for each feature dimension.\n",
    "�\n",
    "γ (scale parameter) and \n",
    "�\n",
    "β (shift parameter) allow the network to learn the optimal scaling and shifting of the normalized activations. This enables the model to maintain representational capacity and flexibility.\n",
    "The final output \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  of the batch normalization operation is obtained by scaling and shifting the normalized activations:\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "^\n",
    "�\n",
    "+\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    " =γ \n",
    "x\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " +β\n",
    "Effectiveness:\n",
    "\n",
    "By normalizing the activations, batch normalization reduces the internal covariate shift, which refers to the change in the distribution of network activations as the parameters of the network are updated during training. This helps stabilize the training process and accelerates convergence.\n",
    "Batch normalization acts as a form of regularization by adding noise to the network's activations, which can reduce overfitting and improve generalization performance.\n",
    "Furthermore, batch normalization helps mitigate the vanishing and exploding gradient problems by stabilizing the gradients during backpropagation.\n",
    "Usage:\n",
    "\n",
    "Batch normalization is typically applied to the activations of each layer in a neural network, usually before the activation function.\n",
    "It is widely used in various types of neural network architectures, including fully connected networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37803e5d",
   "metadata": {},
   "source": [
    "# Q2. Implementation:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d168e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\91779\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91779\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f659eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91779\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\91779\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\91779\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1875/1875 [==============================] - 10s 4ms/step - loss: 0.2587 - accuracy: 0.9264 - val_loss: 0.1418 - val_accuracy: 0.9582\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1138 - accuracy: 0.9654 - val_loss: 0.1130 - val_accuracy: 0.9668\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0776 - accuracy: 0.9769 - val_loss: 0.0887 - val_accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0585 - accuracy: 0.9819 - val_loss: 0.0882 - val_accuracy: 0.9746\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0439 - accuracy: 0.9868 - val_loss: 0.0772 - val_accuracy: 0.9761\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2525 - accuracy: 0.9268 - val_loss: 0.1331 - val_accuracy: 0.9594\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1253 - accuracy: 0.9635 - val_loss: 0.1002 - val_accuracy: 0.9694\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0940 - accuracy: 0.9713 - val_loss: 0.0825 - val_accuracy: 0.9738\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0737 - accuracy: 0.9780 - val_loss: 0.0783 - val_accuracy: 0.9757\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0631 - accuracy: 0.9801 - val_loss: 0.0771 - val_accuracy: 0.9759\n",
      "Baseline Model - Loss: 0.07721029222011566, Accuracy: 0.9761000275611877\n",
      "Model with Batch Normalization - Loss: 0.0770544707775116, Accuracy: 0.9758999943733215\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple feedforward neural network without batch normalization\n",
    "model_baseline = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the baseline model\n",
    "model_baseline.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "model_baseline.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Define a simple feedforward neural network with batch normalization\n",
    "model_bn = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model with batch normalization\n",
    "model_bn.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model_bn.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the models\n",
    "baseline_loss, baseline_acc = model_baseline.evaluate(x_test, y_test, verbose=0)\n",
    "bn_loss, bn_acc = model_bn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline Model - Loss: {}, Accuracy: {}'.format(baseline_loss, baseline_acc))\n",
    "print('Model with Batch Normalization - Loss: {}, Accuracy: {}'.format(bn_loss, bn_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7beef",
   "metadata": {},
   "source": [
    "# Q3. Experimentation and Analysis:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d53e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2526 - accuracy: 0.9279 - val_loss: 0.1251 - val_accuracy: 0.9629\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1257 - accuracy: 0.9629 - val_loss: 0.0939 - val_accuracy: 0.9703\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0935 - accuracy: 0.9718 - val_loss: 0.0890 - val_accuracy: 0.9729\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0748 - accuracy: 0.9779 - val_loss: 0.0757 - val_accuracy: 0.9767\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0604 - accuracy: 0.9812 - val_loss: 0.0744 - val_accuracy: 0.9759\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 6s 5ms/step - loss: 0.2629 - accuracy: 0.9266 - val_loss: 0.1393 - val_accuracy: 0.9598\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1206 - accuracy: 0.9663 - val_loss: 0.1005 - val_accuracy: 0.9699\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0866 - accuracy: 0.9758 - val_loss: 0.0878 - val_accuracy: 0.9719\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0670 - accuracy: 0.9800 - val_loss: 0.0789 - val_accuracy: 0.9760\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0523 - accuracy: 0.9843 - val_loss: 0.0793 - val_accuracy: 0.9758\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 6ms/step - loss: 0.2950 - accuracy: 0.9182 - val_loss: 0.1535 - val_accuracy: 0.9575\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1357 - accuracy: 0.9623 - val_loss: 0.1163 - val_accuracy: 0.9671\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9736 - val_loss: 0.1012 - val_accuracy: 0.9697\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0722 - accuracy: 0.9802 - val_loss: 0.0889 - val_accuracy: 0.9714\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0564 - accuracy: 0.9843 - val_loss: 0.0882 - val_accuracy: 0.9734\n",
      "Epoch 1/5\n",
      "235/235 [==============================] - 3s 8ms/step - loss: 0.3611 - accuracy: 0.9003 - val_loss: 0.2002 - val_accuracy: 0.9477\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1586 - accuracy: 0.9569 - val_loss: 0.1492 - val_accuracy: 0.9560\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1135 - accuracy: 0.9700 - val_loss: 0.1121 - val_accuracy: 0.9682\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0877 - accuracy: 0.9768 - val_loss: 0.0987 - val_accuracy: 0.9698\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0695 - accuracy: 0.9818 - val_loss: 0.0891 - val_accuracy: 0.9731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a list of batch sizes to experiment with\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Define and compile the model with batch normalization\n",
    "    model_bn = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model_bn.compile(optimizer='adam',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model with the current batch size\n",
    "    model_bn.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2c5a2",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "Accelerated Convergence: Batch normalization can speed up the convergence of neural networks by reducing the internal covariate shift, allowing for faster training.\n",
    "Improved Stability: Batch normalization helps stabilize the training process by reducing the likelihood of vanishing or exploding gradients, making it easier to train deeper networks.\n",
    "Regularization: Batch normalization acts as a form of regularization, reducing the need for other regularization techniques like dropout, thus simplifying the training process.\n",
    "Robustness to Hyperparameters: Batch normalization is relatively insensitive to the choice of learning rate and weight initialization, making it easier to tune hyperparameters.\n",
    "\n",
    "\n",
    "Potential Limitations:\n",
    "Increased Computational Overhead: Batch normalization adds computational overhead during both training and inference, as it requires additional calculations and memory.\n",
    "Dependency on Batch Size: The effectiveness of batch normalization can depend on the batch size used during training. Smaller batch sizes may introduce noise in the estimated batch statistics, affecting the performance of batch normalization.\n",
    "Sensitivity to Learning Rate: While batch normalization can make training more robust to the choice of learning rate, excessively high learning rates can still lead to unstable training dynamics.\n",
    "Difficulty in Transfer Learning: Batch normalization layers may need to be retrained or fine-tuned when transferring a model to a new dataset or task, as the statistics of the new data may differ from those of the original training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
